# this model estimates the probability of considering covenant adoption
# and then given adoption the payment they are willing to
# accept for adoption and the proportion of their property they would covenant
model {
  # likelihood for the probability of considering adoption
  # loop through all survey participants
  for (i in 1:N) {
    ACCEPT[i] ~ dbern(px[i])
    logit(px[i]) <- sum(X[i,] * beta_x) + xsa1[SAIDx[i]]
  }
  # loop through SA1s
  for (i in 1:NSA) {
    xsa1[i] <- sum(XSA[i,] * betasa_x)
  }

  # likelihood for willingness to accept values
  # loop through all survey participants who are willing to consider adoption
  for (i in 1:M) {
    CENS[i] ~ dinterval(WTA[i], LIM)
    WTA[i] ~ dnorm(muy[i], tau)
    muy[i] <- sum(Y[i,] * beta_y) + ysa1[SAIDy[i]]
  }
  # loop through SA1s
  for (i in 1:MSA) {
    ysa1[i] <- sum(YSA[i,] * betasa_y)
  }

  # likelihood for the proportion of property to apply covenant
  # loop through all survey participants who are willing to consider adoption
  for (i in 1:O) {
    # parameters for beta distribution
    # mean: mu = a / (a + b)
    # precision: phi = a + b
    # hence: a = mu * phi and b = (1 - mu) * phi
    # expectation depends on predictor variables, precision assumed constant
    PROP[i] ~ dbeta(a[i], b[i])
    a[i] <- muz[i] * phi
  	b[i] <- (1 - muz[i]) * phi
    logit(muz[i]) <- sum(Z[i,] * beta_z) + zsa1[SAIDz[i]]
  }
  # loop through SA1s
  for (i in 1:OSA) {
    zsa1[i] <- sum(ZSA[i,] * betasa_z)
  }

  # Priors
  # uses the approach of Ray-Bing Chen, Chi-Hsiang Chu, Shinsheng Yuan & Ying Nian Wu (2016)
  # Bayesian Sparse Group Selection, Journal of Computational and Graphical Statistics, 25:3,
  # 665-683 for setting priors for variable selection

  # priors for variable inclusion
  p_g <- 0.5
  p_gl <- 0.5

  # property level

  # variance components
  tau <- sig^-2
  sig ~ dlnorm(0, 0.001)
  phi ~ dlnorm(0, 0.001)

  # coefficients

  # variable inclusion
  for (i in 1:NCAT) {
    eta_g_x[i] ~ dbern(p_g)
    eta_g_y[i] ~ dbern(p_g)
    eta_g_z[i] ~ dbern(p_g)
  }

  for (i in 1:NCAT) {
    for (j in CATFROM[i]:CATTO[i]) {
      # X model
      gamma_gl_x[j] ~ dbern(p_gl)
      gammac_gl_x[j] <- (1 - eta_g_x[CATIND[j]]) * 0 + (eta_g_x[CATIND[j]] * gamma_gl_x[j])
      b_x[j] ~ dnorm(0, 1) # tau fixed but could tune using cross-validation or set inverse-gamma hyperparameter (see Chen et al. 2016)
      beta_x[j] <- (1 - (eta_g_x[CATIND[j]] * gammac_gl_x[j])) * 0 + (eta_g_x[CATIND[j]] * gammac_gl_x[j] * b_x[j])
      # Y model
      gamma_gl_y[j] ~ dbern(p_gl)
      gammac_gl_y[j] <- (1 - eta_g_y[CATIND[j]]) * 0 + (eta_g_y[CATIND[j]] * gamma_gl_y[j])
      b_y[j] ~ dnorm(0, 1) # tau fixed but could tune using cross-validation or set inverse-gamma hyperparameter (see Chen et al. 2016)
      beta_y[j] <- (1 - (eta_g_y[CATIND[j]] * gammac_gl_y[j])) * 0 + (eta_g_y[CATIND[j]] * gammac_gl_y[j] * b_y[j])
      # Z model
      gamma_gl_z[j] ~ dbern(p_gl)
      gammac_gl_z[j] <- (1 - eta_g_z[CATIND[j]]) * 0 + (eta_g_z[CATIND[j]] * gamma_gl_z[j])
      b_z[j] ~ dnorm(0, 1) # tau fixed but could tune using cross-validation or set inverse-gamma hyperparameter (see Chen et al. 2016)
      beta_z[j] <- (1 - (eta_g_z[CATIND[j]] * gammac_gl_z[j])) * 0 + (eta_g_z[CATIND[j]] * gammac_gl_z[j] * b_z[j])
    }
  }

  # continuous variables
  for (i in (NCOVCAT + 1):NCOV) {
    # X model
    gamma_gl_x[i] ~ dbern(p_gl)
    b_x[i] ~ dnorm(0, 1) # tau fixed but could tune using cross-validation or set inverse-gamma hyperparameter (see Chen et al. 2016)
    beta_x[i] <- (1 - gamma_gl_x[i]) * 0 + (gamma_gl_x[i] * b_x[i])
    # Y model
    gamma_gl_y[i] ~ dbern(p_gl)
    b_y[i] ~ dnorm(0, 1) # tau fixed but could tune using cross-validation or set inverse-gamma hyperparameter (see Chen et al. 2016)
    beta_y[i] <- (1 - gamma_gl_y[i]) * 0 + (gamma_gl_y[i] * b_y[i])
    # Z model
    gamma_gl_z[i] ~ dbern(p_gl)
    b_z[i] ~ dnorm(0, 1) # tau fixed but could tune using cross-validation or set inverse-gamma hyperparameter (see Chen et al. 2016)
    beta_z[i] <- (1 - gamma_gl_z[i]) * 0 + (gamma_gl_z[i] * b_z[i])
  }

  # SA1 level

  # coefficients - currently assumes only continuous variables at the SA1 level

  # X model
  gammasa_gl_x[1] <- 1
  bsa_x[1] ~ dnorm(0, 0.1)
  betasa_x[1] <- bsa_x[1]
  # Y model
  gammasa_gl_y[1] <- 1
  bsa_y[1] ~ dnorm(0, 0.1)
  betasa_y[1] <- bsa_y[1]
  # Z model
  gammasa_gl_z[1] <- 1
  bsa_z[1] ~ dnorm(0, 0.1)
  betasa_z[1] <- bsa_z[1]
  # loop through predictors
  for (i in 2:NCOVSA) {
    # X model
    gammasa_gl_x[i] ~ dbern(p_gl)
    bsa_x[i] ~ dnorm(0, 1) # tau fixed but could tune using cross-validation or set inverse-gamma hyperparameter (see Chen et al. 2016)
    betasa_x[i] <- (1 - gammasa_gl_x[i]) * 0 + (gammasa_gl_x[i] * bsa_x[i])
    # Y model
    gammasa_gl_y[i] ~ dbern(p_gl)
    bsa_y[i] ~ dnorm(0, 1) # tau fixed but could tune using cross-validation or set inverse-gamma hyperparameter (see Chen et al. 2016)
    betasa_y[i] <- (1 - gammasa_gl_y[i]) * 0 + (gammasa_gl_y[i] * bsa_y[i])
    # Z model
    gammasa_gl_z[i] ~ dbern(p_g)
    bsa_z[i] ~ dnorm(0, 1) # tau fixed but could tune using cross-validation or set inverse-gamma hyperparameter (see Chen et al. 2016)
    betasa_z[i] <- (1 - gammasa_gl_z[i]) * 0 + (gammasa_gl_z[i] * bsa_z[i])
  }
}
